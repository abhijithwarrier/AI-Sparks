{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b5cb2ccf170957b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Programmer: python_scripts (Abhijith Warrier)\n",
    "\n",
    "**PYTHON SCRIPT TO _GENERATE TEXT WITH GPT‚Äë2 USING HUGGING FACE TRANSFORMERS (NO API KEY)_**. üêçüß†üìù\n",
    "\n",
    "This script demonstrates how to use Hugging Face‚Äôs Transformers library to generate text with GPT‚Äë2 / DistilGPT‚Äë2 locally. You‚Äôll install the package, load a text-generation pipeline, customize decoding parameters (temperature, top_k, top_p), and generate multiple completions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26fcecc0769e32e",
   "metadata": {},
   "source": [
    "### üì¶ Install Required Libraries\n",
    "\n",
    "We‚Äôll use transformers (and torch as the backend). No API key needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35439504bcc8a77e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T12:21:02.777900Z",
     "start_time": "2025-08-25T12:21:01.988221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install Hugging Face Transformers and PyTorch (CPU)\n",
    "# If you already have them, you can skip this cell.\n",
    "!pip install transformers torch --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ac987757fa513a",
   "metadata": {},
   "source": [
    "### üß∞ Import & Set Up the Text-Generation Pipeline\n",
    "\n",
    "Use pipeline(\"text-generation\", model=...).\n",
    "distilgpt2 is a lighter, faster variant of GPT‚Äë2‚Äîgreat for demos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40e860a83f0b9094",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T12:22:20.077031Z",
     "start_time": "2025-08-25T12:21:06.891881Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Import the pipeline utility from Transformers\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "# Create a text-generation pipeline with a small, fast model\n",
    "# You can switch to \"gpt2\" for the full model if your machine can handle it.\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "\n",
    "# Set a seed for reproducibility (same outputs across runs)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1514587a43a1e86a",
   "metadata": {},
   "source": [
    "### ‚úçÔ∏è Provide a Prompt\n",
    "\n",
    "This is the starting text the model will continue from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ce03ac96b6d49d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T12:22:28.193045Z",
     "start_time": "2025-08-25T12:22:28.189952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: In the next five years, Python developers will\n"
     ]
    }
   ],
   "source": [
    "# Your starting text\n",
    "prompt = \"In the next five years, Python developers will\"\n",
    "print(\"Prompt:\", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829b3d399a2f4aae",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Configure Decoding Parameters\n",
    "- `max_length`: total tokens (prompt + generated)\n",
    "- `temperature`: higher ‚Üí more randomness (e.g., 0.7‚Äì1.2)\n",
    "- `top_k`/`top_p`: sampling strategies to control diversity\n",
    "- `num_return_sequences`: how many variations to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b97d81f85370e3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T12:32:21.856021Z",
     "start_time": "2025-08-25T12:32:21.850733Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generation configuration\n",
    "gen_kwargs = {\n",
    "    \"max_length\": 60,       # total tokens including prompt\n",
    "    \"temperature\": 0.9,     # creativity; try 0.7‚Äì1.1\n",
    "    \"top_k\": 50,            # sample from top-k tokens\n",
    "    \"top_p\": 0.95,          # or nucleus sampling\n",
    "    \"do_sample\": True,      # enable sampling (not greedy)\n",
    "    \"num_return_sequences\": 2,  # generate multiple candidates\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ab90a34c9a6321",
   "metadata": {},
   "source": [
    "### üöÄ Generate Text\n",
    "\n",
    "Run the pipeline and print each generated sample cleanly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5d2902e4be788ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T12:32:35.174779Z",
     "start_time": "2025-08-25T12:32:23.473460Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Completion 1 ===\n",
      "In the next five years, Python developers will get to use the Python standard with a lot of new features, including new toolbars, new APIs, and more. You can read more on how to use the Python standard in Python.\n",
      "\n",
      "=== Completion 2 ===\n",
      "In the next five years, Python developers will spend the next five years building a framework that can be used on every platform and every platform. This new framework can be used across multiple platforms, including mobile and web browsers. If you were to build a framework, you'd need an existing framework for your Python projects. Today, more than 100,000 software developers are building the framework in their communities and have contributed to the success of this platform.\n",
      "\n",
      "\n",
      "\n",
      "Python‚Äôs community is growing at a rate of 2 million people. From 1.2 million people, to 1.3 million people in 2015, it's an amazing development ecosystem. In 2014, more than 80% of all code is written on Python.\n",
      "For many, Python‚Äôs community is a huge success. Python is now the biggest operating system in the world, and it's the biggest community in the world. The fact that I don't see why anyone could be better than me is a huge shame.\n",
      "You can see more of my articles here.\n"
     ]
    }
   ],
   "source": [
    "# Generate multiple completions\n",
    "outputs = generator(prompt,\n",
    "                    truncation=True,           # handle long prompts explicitly\n",
    "                    pad_token_id=50256,        # set EOS token as padding\n",
    "                    **gen_kwargs)\n",
    "\n",
    "# Display results\n",
    "for i, out in enumerate(outputs, 1):\n",
    "    print(f\"\\n=== Completion {i} ===\")\n",
    "    print(out[\"generated_text\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbadefd59fad5d0f",
   "metadata": {},
   "source": [
    "### üí° Tips & Variations (Optional)\n",
    "\n",
    "Try these quick tweaks to change style and length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e343c6ba5b48a6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T12:18:02.916152Z",
     "start_time": "2025-08-25T12:18:02.913653Z"
    }
   },
   "outputs": [],
   "source": [
    "# (Optional) Try a different model\n",
    "# generator = pipeline(\"text-generation\", model=\"gpt2\")  # larger, better but heavier\n",
    "\n",
    "# (Optional) Make the text shorter/longer\n",
    "# gen_kwargs[\"max_length\"] = 120  # longer generations\n",
    "\n",
    "# (Optional) Make it more/less creative\n",
    "# gen_kwargs[\"temperature\"] = 1.1  # more random\n",
    "# gen_kwargs[\"temperature\"] = 0.7  # more focused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f7b7edc93eab9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c421fec6-b27a-4a60-bdcb-48099d7f50d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
